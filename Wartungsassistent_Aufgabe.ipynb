{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd440299-0562-4744-9fa3-acf638f097a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "# Modul 08 Large Language Models und Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26292140",
   "metadata": {},
   "source": [
    "## Das Szenario \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> Das Unternehmen Alstrom produziert Straßenbahnen. Sie sind neuen Technologien gegenüber sehr aufgeschlossen und haben die Idee einen <b> intelligenten Wartungsassistent </b> für die Produktionslinie B zu entwickeln.\n",
    "\n",
    "<b>Ziel:</b> Der Assistent soll Technikern bei der <b> Fehlerdiagnose und Wartung von Fertigungsrobotern </b> und Montageanlagen helfen. Die Techniker geben ihre Beobachtung oder Fehldercodes in ein Chatfenster ein und der Wartungsassistent gibt Ihnen eine Handlungsempfehlung. \n",
    "\n",
    "Der Manager schlägt vor, <b>ChatGPT</b> für diese Aufgabe zu nutzen, während der Meister sagt, dass es ein <b>RAG-System</b> brauchst.\n",
    "\n",
    "<b> Deine Aufgabe im Laufe dieses Notebooks ist es, das RAG-System zu implementieren und dessen Performance mit der von ChatGPT zu vergleichen. Die Ergebnisse deines Vergleichs präsentierst du Meister und Manager. </b>\n",
    "\n",
    " </div>\n",
    "\n",
    "![Platzhalter](pictures/Produktionslinie.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e83b5",
   "metadata": {},
   "source": [
    "## Überblick\n",
    "\n",
    "Dieses Notebook leitet dich durch die notwendigen Schritte. \n",
    "<ul>\n",
    "<li> Die Basis für einen Test erstellen </li>\n",
    "<li> Option A testen und bewerten</li>\n",
    "<li> Option B testen und bewerten</li>\n",
    "<li> Option A und B miteinander vergleichen </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bb5ef",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Die Basis für einen Test erstellen\n",
    "\n",
    "Um die beiden Optionen, welche wir testen sollen, gut miteinander vergleichen zu können und dem Vorgesetzten erklären zu können, welche besser geeignet ist, brauchen wir erstmal einen geeigneten Maßstab. Das ist vergleichbar mit den <i> Performance Metriken, </i> die du aus den anderen Modulen bereits kennst. \n",
    "Beim Arbeiten mit Textdaten heißen diese <i> Benchmarks </i> und sind speziell angefertigte Datensätze. **Eine solche Benchmark wirst du nun selbst erstellen.** \n",
    "\n",
    "Dafür musst du dich ein wenig mit den Daten aus der Produktionslinie B vertraut machen. Im Ordner **Alstrom_Dokumentation** findest du jede Menge Dokumente aus dem Unternehmen.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Sieh dir die Dokumente in dem Order an und überlege dir:\n",
    "<ul>\n",
    "<li> 1. eine Frage, mit welchem du den Wartungsassitent testen willst\n",
    "<li> 2. die ideale Antwort, die der Wartungsassistent geben sollte\n",
    "</ul>\n",
    "\n",
    "Versuche dabei Fragen zu stellen, auf die es <b>konkrete Antworten</b> gibt, z.B Wer muss informiert werden, wenn ... passiert?, Wo finde ich ... ?, etc. </div>\n",
    "\n",
    "\n",
    "### Frage-Antwort-Paar in die Datei <i>Benchmark.json</i> eintragen\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> \n",
    "Um das Paar aus Frage und idealer Antwort in die Benchmark einzutragen, führe die nächste Codezelle aus. Gib im Eingabefeld die Frage ein, bestätige mit <i> Enter</i>; gib dann die richtige Antwort ein und bestätige wieder mit <i> Enter</i>. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba34329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import fill_bench_df\n",
    "\n",
    "bench = fill_bench_df(question=\"\", ground_truth=\"\", GPT_answer=\"\", GPT_score=\"\", RAG_answer=\"\", RAG_relevance=\"\", RAG_use=\"\", RAG_completeness=\"\" )\n",
    "print(bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aec36c",
   "metadata": {},
   "source": [
    "### Prüfen ob die Eintragung funktioniert hat\n",
    "\n",
    "![Platzhalter](pictures/Explorer.png)  \n",
    "\n",
    "\n",
    "Im VSCode-Explorer siehst du nun ein Dokument namens <i>benchmark.json</i>. Klicke darauf, um es zu öffnen.\n",
    "\n",
    "![Platzhalter](pictures/JC_Button.png)\n",
    "\n",
    "Klicke dann auf den Button <i> JC</i> oben rechts. Dann erhälts du eine übersichtliche Darstellung deines Eintrags.\n",
    "\n",
    "![Platzhalter](pictures/json.png)\n",
    "\n",
    "\n",
    "\n",
    "## 2. Option A (ChatGPT) testen\n",
    "\n",
    "Nun geht es darum Option A zu testen. D.h. zu prüfen, wie **ChatGPT** die von dir ausgewählte Frage beantwortet. Die Antwort und deine Bewertung trägst du anschließend auch in die Benchmark ein, um sie später besser präsentieren zu können.\n",
    "\n",
    "![Platzhalter](pictures/LLM_diagramm.png)\n",
    "\n",
    "Nachdem ChatGPT schon ein fertiges Produkt ist, musst du nicht mehr viel vorbereiten. Auf dem anderen Bildschirm siehst du bereits einen Tab geöffnet, in diesem kannst du mit dem Modell **ChatGPT** chatten. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Stelle nun die Frage aus deiner Benchmark an ChatGPT. </div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Um die Antworten von ChatGPT in deinen Datensatz zu integrieren, führe die nächste Codezelle aus. Gibt dann zunächst die <b>id </b> der Frage ein, bestätige mit <i> Enter</i>; kopiere dann die Antwort von <b> ChatGPT</b> aus dem Terminal und füge sie in die Eingabezeile ein. Bestätige wieder mit <i> Enter</i>. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3117dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_GPT_answer\n",
    "bench = insert_GPT_answer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c3dd1",
   "metadata": {},
   "source": [
    "### Qualitative Bewertung\n",
    "\n",
    "In deinem Datensatz hast du nun die richtige Antwort und die Antwort von ChatGPT nebeneinander stehen. Sieh dir die beiden Antworten an und bewerte die Performance von ChatGPT. Du kannst zwischen 0 und 10 Punkten vergeben. Wobei 0 Punkte sehr schlecht ist und 10 Punkte eine ideale Antwort. Verwende für die Bewertung auch deine vorher definierte **richtige Antwort**.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Indem du die folgende Codezelle ausführst kannst du die Bewertung wieder in die Benchmark eintragen.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_GPT_score\n",
    "bench = insert_GPT_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd2a0e-3555-4a80-ba97-10f704ac8410",
   "metadata": {},
   "source": [
    "## 3. Option B (RAG) testen\n",
    "\n",
    "Nun geht es darum Option B zu testen. Nachdem sich die technischen Informationen oder auch Zuständigkeiten schnell ändern können, hat der Meister vorgeschlagen RAG (Retrieval Augmented Generation) zu nutzen. \n",
    "Der große Unterschied zwischen einem klassischen LLM und einem LLM mit RAG ist der Weg, den der Promt des Users durchläuft. Statt direkt an das LLM gegeben zu werden, wird der Promt einer Suchfunktion übergeben. Diese sucht in einer Datenbank nach relevaten Information, die mit dem Prompt dann an das LLM übergeben werden. \n",
    "\n",
    "![Platzhalter](pictures/RAG_LLM_diagramm.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> In den nächsten Schritten wirst du diesen Weg definieren.  \n",
    " <b> Den Assistenten der dabei entsteht nennen wir  <i> RAG-Bot </i>. </b> Wenn er fertig zusammengebaut ist, können wir dessen Performance testen. </div>\n",
    "\n",
    " \n",
    "\n",
    "### 3.1 Installieren von Software-Paketen\n",
    "\n",
    "Der erste Schritt beim Implementieren ist immer das Installieren der richtigen Software-Pakete (auch Bibliotheken genannt). \n",
    "\n",
    "Die Bibliothek, die wir für unseren RAG-Bot verwenden, heißt <strong>llama-index </strong>. Die Dokumentation der Bibliothek findest du hier:  <a> https://docs.llamaindex.ai/en/stable/ </a>. Für die Arbeit mit dem Notebook während der Praxistage benötigst du die Dokumentation allerdings nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdc1d5-716a-48c0-bc0d-98d0569f9a46",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> Dies ist ein einmaliger Schritt. D.h er muss nur ausgeführt werden, wenn das Notebook auf einem neuen Computer läuft. Du kannst also direkt zum Importieren von Funktionen übergehen. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec6e12-e2a8-436b-ac3d-1bc1832f4a28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFwm7wDSoF3V",
    "outputId": "991ac34d-4f12-440b-968a-a2e23df797ec"
   },
   "outputs": [],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aadaeb-e923-44a9-b18c-c227a4b63a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "## 3.2 Importieren von Funktionen\n",
    "\n",
    " \n",
    "Um einzelne Funktionen der Bibliothek nutzen zu können, müssen wir die benötigten Elemente von <strong>llama-index </strong> importieren. Die 5 Elemente <i> Settings, VectorStoreIndex, SimpleDirectoryReader, HuggingFaceEmbedding und ollama </i> werden wir im Laufe des Notebooks für unseren RAG-Bot verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f7f94-6566-4288-8cae-d74169fac6a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus, um die benötigten Bibliotheken zu importieren.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "#from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40414e",
   "metadata": {},
   "source": [
    "## 3.3 Suche nach relevanten Daten\n",
    "\n",
    "Nun geht es darum, nach relevanten Dokumenten zu suchen. Denn damit unser Wartungsassistent <i>hilfreiche Informationen zur Produktionslinie B </i> geben kann, müssen wir erst einmal entscheiden, was hilfreiche Informationen für diesen Anwendungsfall sind. \n",
    "\n",
    "Im Ordner **Alstrom_Dokumentation** findest du jede Menge Dokumente aus dem Unternehmen.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Sieh dir die Dokumente im Ordner <b>Alstrom_Dokumentation</b> an und entscheide, welche für den Wartungsassistenten sinnvoll oder notwendig sind! Ziehe diese in den Ordner <b>Wissensbasis</b>.\n",
    "</div>\n",
    "\n",
    "\n",
    "![Platzhalter](pictures/Explorer1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904afef9",
   "metadata": {},
   "source": [
    "## 3.4 Einlesen der Daten\n",
    "\n",
    "Eben hast du entschieden, welche Dokumente für den RAG-Bot relevant sind und diese in den Ordner <i> Wissensbasis </i> verschoben. Für die Verwendung mit RAG ist es aber auch wichtig, dass die Dokumente durchsuchbar sind. \n",
    "\n",
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b>QUIZ:</b> Führe die nächste Codezelle aus, um die Quizfrage anzuzeigen. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "from helpers.q_helpers import load_question\n",
    "q4 = load_question('q4')\n",
    "q5 = load_question('q5')\n",
    "display_quiz([q4, q5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa925c9c",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Überprüfe alle Dokumente im Ordner Wissensbasis auf Durchsuchbarkeit und passe deine Auswahl ggf. an.</div>\n",
    "\n",
    "Mit der nächsten Codezeile erhält der RAG-Bot Zugriff auf die Dateien, welche in dem Ordner <i> Wissensbasis </i> gespeichert sind. Dafür muss der Ordner natürlich am richtigen Ort sein. Alle Dateien in diesem Ordner werden mit dem Objekt **documents** gleichgesetzt. So kann es anderen Funktionen zur Verfügung gestellt werden.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = SimpleDirectoryReader(\"Wissensbasis\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fad56",
   "metadata": {},
   "source": [
    "\n",
    "## 3.5 Embedding\n",
    "\n",
    "Vorhin haben wir sichergestellt, dass die Dokumente in unserer Wissensbasis durchsuchbar sind. Das heißt eine Suchfunktion kann Wörter und Sätze aus den Dokumenten entnehmen. Trotzdem liegen die Texte darin noch nicht im richtigen Format für ein LLM vor.<strong> Denn Neuronale Netze brauchen numerischen Input! </strong> Wir müssen also noch einen Umwandlungsschritt machen: Jedes einzele enthaltene Wort muss in einen Vektor umgewandelt werden. \n",
    "\n",
    "![Platzhalter](../RAG/pictures/Embedding.png)\n",
    "\n",
    "Dafür werden bereits trainierte Embedding Modelle verwendet. Die meisten Embedding Modelle sind einsprachig. Einige große Modelle sind mehrsprachig, können aber trotzdem meistens eine besonders gut. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus, um ein <b> Embedding-Modell </b> von der Plattform Huggingface zu importieren. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07af3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"mixedbread-ai/deepset-mxbai-embed-de-large-v1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3625ea",
   "metadata": {},
   "source": [
    "## 3.6 Speichern in Vektordatenbank\n",
    "\n",
    "Nachdem das Embeddingmodell geladen wurde, kann es auf den Text in unserem **documents** Objekt angewendet werden. \n",
    "Mit der nächsten Codezeile passiert aber noch etwas mehr:\n",
    "\n",
    "<ul>\n",
    "<li> Jedes Wort wird ein Vektor zugeordnet (wie in der Abbildung oben) </li>\n",
    "<li> Der ganze Text wird in kleine Schnipsel (Chunks) aufgeteilt. Default: 1024 Zeichen pro Chunk </li>\n",
    "<li> Die Chunks werden dann wieder in einem Vektorraum (VektorStoreIndex) gespeichert. Dadurch sind sie für die Suchfunktion, die wir gleich zusammensatzen, leichter durchsuchbar. </li>\n",
    "</ul>\n",
    "\n",
    "Grob können wir uns also vorstellen, dass jedes Chunk einen Punkt im Vektorraum zugeordnet bekommt. Auf diese Weise können die Chunks später mit Anfragen an das Modell verglichen werden.\n",
    "\n",
    "![Platzhalter](pictures/Embedding_chunks.png)\n",
    "\n",
    " Hier ein Beispiel, wie die Suchfunktion ein Mapping zwischen einer Useranfrage und den Chunks der Wissensdatenbank herstellt:\n",
    " Wenn ein User eine Frage stellt, werden die einzelnen Worte auch durch das Embedding Modell in Vektoren umgewandelt. Die ganze Frage wird an einem Ort in der *VectorDataBase* platziert. Der Stern repräsentiert hier die Useranfrage. Durch den K-Nächste-Nachbarn Algorithmus (KNN) wird bestimmt, welche Chunks aus der Datenbank abgefragt werden. Im Bild durch den Kreis gezeigt, siehst du welche Chunks bei k = 3 ausgewählt werden.\n",
    "\n",
    "![Platzhalter](pictures/Embedding_search_.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag: </b>Führe die nächste Codezelle aus, um die Vektordatenbak zu befüllen.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c09807",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c66a2",
   "metadata": {},
   "source": [
    "## 3.7 Importieren des LLMs\n",
    "\n",
    "Nun laden wir das firmeneigene LLM **Phi3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c40975",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste codezelle aus, um das LLM zu laden. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97224d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"phi3\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14067d3d-62f0-4784-8f15-7bd304cf8252",
   "metadata": {},
   "source": [
    "## 4 Zusammensetzen und Ausprobieren\n",
    "\n",
    "Die Nächste Codezeile setzt nun alle Elemente, die du bisher vorbereitet hast, zusammen. D.h.: Es ist Zeit zum Ausprobieren! \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Teste die Fragen aus deiner Benchmark nun am RAG-Bot. <b> Wenn du die nächste Codezelle ausfüllst, sieh an den oberen Bildschirmrand. Dort erscheint ein Feld, indem du die Frage eingeben kannst! </b> Bestätige mit Enter, um die Frage abzuschicken. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from helpers.query_helpers import set_query\n",
    "TASK = 'WRITE'\n",
    "\n",
    "\n",
    "#debug_handler = LlamaDebugHandler()\n",
    "#callback_manager = CallbackManager([debug_handler])\n",
    "query_engine = index.as_query_engine(\n",
    "    #callback_manager = callback_manager\n",
    ")\n",
    "response = query_engine.query(set_query(TASK))\n",
    "formatted_response= textwrap.fill(response.response, width=80)\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0168f39",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Um die Antworten von RAG-Bot in deinen Datensatz zu integrieren, führe die nächste Codezelle aus. Gibt dann zunächst die <b>id </b> der Frage ein, bestätige mit <i> Enter</i>; kopiere dann die Antwort vom <b> Wartungsassistenten</b> aus dem Terminal und füge sie in die Eingabezeile ein. Bestätige wieder mit <i> Enter</i>. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7341de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_rag_answer\n",
    "bench = insert_rag_answer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50a9ca",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "Um die Performance von RAG-Systemen zu prüfen und gleichzeitig Informationen darüber zu erhalten, wo im RAG-System nachgebessert werden muss, kann man z.B. die folgenden 4 Maßstäbe nutzen:\n",
    "<ul>\n",
    "<li> Relevanz = werden nützliche Dokumente gefunden?\n",
    "<li> Nutzung = werden die gefundenen Infos genutz?\n",
    "<li> Treue = basiert die Antwort <b> nur </b> auf den Dokumenten?\n",
    "<li> Vollständigkeit = sind alle Infos enthalten?\n",
    "</ul>\n",
    "\n",
    "Die Evaluation in unterschiedliche Kriterien aufzuteilen macht deswegen Sinn, weil es beim Beheben von Fehlern unterstützt. Zur Erinnerung: ein RAG-System besteht aus unterschiedlichen Komponenten. An jeder dieser Komponenten können Veränderungen vorgenommen werden, die zu einer besseren oder schlechteren Performance führen. \n",
    "\n",
    "![Platzhalter](pictures/RAG_diagramm_eval.png)\n",
    "\n",
    "\n",
    "Die Antwort von RAG-Bot allein ist für diese Bewertung jedoch nicht ausreichend. Du musst auch überprüfen welche Textdateien in der Datenbank gefunden und zum Generieren der Antwort verwendet wurden. \n",
    "\n",
    "Im nächsten Bild siehst du ein Beispiel: Gelb markiert sind die <b>Textdateien die verwendet wurden</b>, blau markiert ist die daraus erzeugte Antwort. \n",
    "\n",
    "![Platzhalter](pictures/retrival_insights.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Führe die nächste Codezelle aus um Informationen über die gefundenen Texte zu bekommen.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e873075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "formatted_insights = pprint.pformat(response.__dict__)\n",
    "print(formatted_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4677084",
   "metadata": {},
   "source": [
    "### 5.1 Relevanz\n",
    "\n",
    "Mit diesen Informationen kannst du eine Bewertung in der Kategorie <b>Relevanz</b> vornehmen. Bewerte ob die ausgewählten Texte relevante Information in Bezug auf deine Frage enthalten.\n",
    "\n",
    "\n",
    " <div class=\"alert alert-block alert-success\"> Indem du die nächste Codezelle ausführst, kannst du deine Bewertung wieder in deine Benchmark eintragen. Nutze hierfür die Kategorien: <b>relevant</b>, <b>teils relevant</b> und <b>nicht relevant</b>.</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d86362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_rag_relevance\n",
    "bench = insert_rag_relevance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b0ced",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Info:</b> Wenn sich beim Benchmarking z.B. herausstellt, dass relevante Dokumente sehr selten gefunden werden, ist es ein sinnvoller Schritt die Suchfunktion, das Embedding Modell und die Datenbank zu überprüfen. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0f7c1",
   "metadata": {},
   "source": [
    "### 5.2 Nutzung und Treue\n",
    "\n",
    "Nun musst du prüfen ob die Texte, welche gefunden wurden auch wirklich für die Generierung der Antwort verwendet wurden. Denn auch an diesem Schritt können Fehler im Prozess entstehen. Die kannst du tun indem du die generierte Antwort mit den Inhalten der gefundenen Texte vergleichst.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> Lese dir die Antwort von RAG-Bot <b>sowie</b> die gefundenen Texte <b> genau </b> durch und vergleiche. Hat sich RAG-Bot bei der Generiereung der Antwort steng an die Inhalte der Texte gehalten? Beurteile Nutzung und Treue der Antwort wieder von 0 bis 10 Punkten! \n",
    "\n",
    "Mit der nächsten Codezelle kannst du deine Bewertung in die Benchmark eintragen.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_rag_use\n",
    "bench = insert_rag_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5915766",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Info:</b> Ergibt das Benchmarking, dass sich das LLM beim generieren der Antwort selten an die Inhalte in den Bereitgestellten texten hält kann folgendes geprüft werden:\n",
    "<ul>\n",
    "<li> Ein klarer Systemprompt der immer mit der Useranfrage und dem Kontext übergeben wird.</li>\n",
    "<li> Eine Beschränkung der ausgewählten Texte </li>\n",
    "<li> Falls möglich: Das Einstellen von Parametern am LLM, welches es weniger <it>\"kreativ\"</it> macht.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "### 5.3 Vollständigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f598",
   "metadata": {},
   "source": [
    "\n",
    "Der nächste Punkt der Bewertung ist die Vollständigkeit. Prüfe ob die Antwort von RAG-Bot alle Infos enthält was für eine optimale Antwort wichtig ist. Beziehe dich dabei sowohl auf die <b>von dir zu Beginn festgelegte richtige Antwort</b>, als auch auf die Inhalte in den Textquellen. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Indem du die folgende Codezelle ausführst kannst du die Bewertung wieder in die Benchmark eintragen. Nutze hier die Kategorien <b>vollständig</b>, <b>beinahe vollständig</b> und <b>unvollständig</b>. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import insert_rag_completeness\n",
    "bench = insert_rag_completeness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927ac0d",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Info:</b> Auch wenn die Textquellen optimal genutz wurden kannst du Unvollstädndigkeit feststellen. z.B. dann wenn Dokumente in der Wissensdatenbank fehlen. \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    " <b>Nun hast du den Testlauf mit einer Frage durchlaufen. Eine Frage ist aber nicht genug für die Präsentation. Dafür benötigst du heute mindestens 3 Fragen.</b>\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> \n",
    "Wiederhole die <b> Schritte 1, 2, 4 und 5 </b> so lange, bis du <b> mindestens 3 Fragen mit Bewertungen </b> in den Datensatz eingetragen hast. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31700",
   "metadata": {},
   "source": [
    "<div style=\"display:none\" >\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, node in enumerate(debug_handler.get_retrived_nodes()):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Text: {node.text}\")\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
